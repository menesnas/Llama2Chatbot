import os
import arxiv
from libraries import *

# ArXiv paper ID'leri - LLM konusunda Ã¶nemli makaleler
ARXIV_PAPERS = [
    '2308.10620',  # LLaMA 2
    '2307.06435',  # Llama 2 Chat
    '2303.18223',  # GPT-4
    '2307.10700',  # Code Llama
    '2310.11207',  # Mistral 7B
    '2305.11828',  # Vicuna
    '2307.09288',  # Longformer
    '2305.15334',  # PaLM 2
    '1706.03762',  # Attention is all you need
]

def download_arxiv_papers():
    """ArXiv'den makaleleri indir"""
    
    data_dir = "./data"
    
    # Data klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    os.makedirs(data_dir, exist_ok=True)
    print(f"ğŸ“ Data klasÃ¶rÃ¼ hazÄ±r: {data_dir}")
    
    downloaded_papers = []
    
    print(f"ğŸ“š {len(ARXIV_PAPERS)} ArXiv makalesi kontrol ediliyor...")
    
    for i, paper_id in enumerate(ARXIV_PAPERS, 1):
        filename = f"{paper_id}.pdf"
        file_path = os.path.join(data_dir, filename)
        
        # Dosya zaten varsa atla
        if os.path.exists(file_path) and os.path.getsize(file_path) > 100000:  # En az 100KB
            print(f"âœ… ({i}/{len(ARXIV_PAPERS)}) {filename} zaten mevcut")
            downloaded_papers.append(file_path)
            continue
        
        try:
            print(f"ğŸ“¥ ({i}/{len(ARXIV_PAPERS)}) {paper_id} indiriliyor...")
            
            # ArXiv'den makale al
            search = arxiv.Search(id_list=[paper_id])
            paper = next(search.results())
            
            # PDF'i indir
            paper.download_pdf(dirpath=data_dir, filename=filename)
            
            if os.path.exists(file_path) and os.path.getsize(file_path) > 100000:
                print(f"âœ… BaÅŸarÄ±lÄ±: {filename}")
                downloaded_papers.append(file_path)
            else:
                print(f"âŒ BaÅŸarÄ±sÄ±z: {filename}")
                
        except Exception as e:
            print(f"âŒ {paper_id} indirilemedi: {e}")
            continue
    
    print(f"ğŸ“ Toplam {len(downloaded_papers)} makale hazÄ±r")
    return downloaded_papers

def load_and_process_documents():
    """Belgeleri yÃ¼kle ve iÅŸle"""
    
    # ArXiv makalelerini indir
    download_arxiv_papers()
    
    dirpath = './data'
    
    # PDF dosyalarÄ±nÄ± kontrol et
    pdf_files = [f for f in os.listdir(dirpath) if f.endswith('.pdf')]
    
    if not pdf_files:
        print("âŒ HiÃ§ PDF bulunamadÄ±!")
        return None
    
    print(f"ğŸ“ {len(pdf_files)} PDF dosyasÄ± bulundu")
    
    # PDF dosyalarÄ±nÄ± yÃ¼kle
    loader = DirectoryLoader(path=dirpath, glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"ğŸ“– Toplam {len(documents)} sayfa yÃ¼klendi")
    
    if not documents:
        print("âŒ HiÃ§ belge yÃ¼klenemedi!")
        return None
    
    # Metinleri parÃ§alara bÃ¶l
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    texts = splitter.split_documents(documents)
    print(f"ğŸ“„ Toplam {len(texts)} metin parÃ§asÄ± oluÅŸturuldu")
    
    # GPU kullanÄ±labilirliÄŸini kontrol et
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ Embedding iÃ§in kullanÄ±lan cihaz: {device}")
    
    # Embedding modelini yÃ¼kle
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': device}
    )
    
    # Vector store veritabanÄ±nÄ± oluÅŸtur
    print("ğŸ”— Vector store oluÅŸturuluyor...")
    db = FAISS.from_documents(texts, embeddings)
    
    # Vector store'u kaydet
    db.save_local("faiss_index")
    print("ğŸ’¾ Vector store kaydedildi!")
    return db

if __name__ == "__main__":
    print("ğŸš€ Sistem baÅŸlatÄ±lÄ±yor...")
    db = load_and_process_documents()
    
    if db:
        print("âœ… Sistem hazÄ±r!")
    else:
        print("âŒ Sistem baÅŸlatÄ±lamadÄ±!")